import os
import json
import time
import re
import random
import requests
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import gspread

# è¨­å®š
KEYWORD = "æ—¥ç”£"  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
# ğŸ“ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã¯GitHub Actionsã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã“ã¨ã‚’æ¨å¥¨
# ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã‚’æœ‰åŠ¹ã«ã™ã‚‹
# SPREADSHEET_ID = "1ZqRekcKkUUoVxZuO8hrWRWwTauyEk8kD_NmV5IZy02w"

def format_datetime(dt_obj):
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def parse_relative_time(pub_label: str, base_time: datetime) -> str:
    pub_label = pub_label.strip().lower()
    try:
        if "åˆ†å‰" in pub_label or "minute" in pub_label:
            m = re.search(r"(\d+)", pub_label)
            if m:
                dt = base_time - timedelta(minutes=int(m.group(1)))
                return format_datetime(dt)
        elif "æ™‚é–“å‰" in pub_label or "hour" in pub_label:
            h = re.search(r"(\d+)", pub_label)
            if h:
                dt = base_time - timedelta(hours=int(h.group(1)))
                return format_datetime(dt)
        elif "æ—¥å‰" in pub_label or "day" in pub_label:
            d = re.search(r"(\d+)", pub_label)
            if d:
                dt = base_time - timedelta(days=int(d.group(1)))
                return format_datetime(dt)
        elif re.match(r'\d+æœˆ\d+æ—¥', pub_label):
            dt = datetime.strptime(f"{base_time.year}å¹´{pub_label}", "%Yå¹´%mæœˆ%dæ—¥")
            return format_datetime(dt)
        elif re.match(r'\d{4}/\d{1,2}/\d{1,2}', pub_label):
            dt = datetime.strptime(pub_label, "%Y/%m/%d")
            return format_datetime(dt)
        elif re.match(r'\d{1,2}:\d{2}', pub_label):
            t = datetime.strptime(pub_label, "%H:%M").time()
            dt = datetime.combine(base_time.date(), t)
            if dt > base_time:
                dt -= timedelta(days=1)
            return format_datetime(dt)
    except:
        pass
    return "å–å¾—ä¸å¯"

def get_last_modified_datetime(url):
    try:
        response = requests.head(url, timeout=5)
        if 'Last-Modified' in response.headers:
            dt = parsedate_to_datetime(response.headers['Last-Modified'])
            jst = dt.astimezone(timedelta(hours=9))
            return format_datetime(jst)
    except:
        pass
    return "å–å¾—ä¸å¯"

def get_google_news_with_selenium(keyword: str) -> list[dict]:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(5)
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("article")
    data = []
    for article in articles:
        try:
            a_tag = article.select_one("a.JtKRv")
            time_tag = article.select_one("time.hvbAAd")
            source_tag = article.select_one("div.vr1PYe")
            title = a_tag.text.strip()
            href = a_tag.get("href")
            url = "https://news.google.com" + href[1:] if href.startswith("./") else href
            dt = datetime.strptime(time_tag.get("datetime"), "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)
            pub_date = format_datetime(dt)
            source = source_tag.text.strip() if source_tag else "N/A"
            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source})
        except:
            continue
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    driver.get(search_url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    articles_data = []

    for article in articles:
        try:
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag else ""
            time_tag = article.find("time")
            date_str = time_tag.text.strip() if time_tag else ""
            formatted_date = ""
            if date_str:
                date_str = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    dt_obj = datetime.strptime(date_str, "%Y/%m/%d %H:%M")
                    formatted_date = format_datetime(dt_obj)
                except:
                    formatted_date = date_str

            source_text = ""
            source_tag = article.find("div", class_="sc-n3vj8g-0 yoLqH")
            if source_tag:
                inner = source_tag.find("div", class_="sc-110wjhy-8 bsEjY")
                if inner and inner.span:
                    candidate = inner.span.text.strip()
                    if not candidate.isdigit():
                        source_text = candidate
            if not source_text or source_text.isdigit():
                alt_spans = article.find_all(["span", "div"], string=True)
                for s in alt_spans:
                    text = s.text.strip()
                    if 2 <= len(text) <= 20 and not text.isdigit() and re.search(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥A-Za-z]', text):
                        source_text = text
                        break

            if title and url:
                articles_data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": formatted_date if formatted_date else "å–å¾—ä¸å¯",
                    "å¼•ç”¨å…ƒ": source_text
                })
        except:
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(articles_data)} ä»¶")
    return articles_data

def get_msn_news_with_selenium(keyword: str) -> list[dict]:
    now = datetime.utcnow() + timedelta(hours=9)
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    url = f"https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d'1'&form=YFNR"
    driver.get(url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    cards = soup.select("div.news-card")
    data = []

    for card in cards:
        try:
            title = card.get("data-title", "").strip()
            url = card.get("data-url", "").strip()
            source = card.get("data-author", "").strip()
            pub_label = ""
            pub_date = ""

            pub_tag = card.find("span", attrs={"aria-label": True})
            if pub_tag and pub_tag.has_attr("aria-label"):
                pub_label = pub_tag["aria-label"].strip().lower()

            pub_date = parse_relative_time(pub_label, now)

            if pub_date == "å–å¾—ä¸å¯" and url:
                pub_date = get_last_modified_datetime(url)

            if title and url:
                data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": pub_date,
                    "å¼•ç”¨å…ƒ": source if source else "MSN"
                })
        except Exception as e:
            print(f"âš ï¸ MSNè¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def write_to_spreadsheet(articles: list[dict], spreadsheet_id: str, worksheet_name: str):
    # èªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
    credentials_json_str = os.environ.get('GCP_SERVICE_ACCOUNT_KEY')
    if credentials_json_str:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰JSONã‚’èª­ã¿è¾¼ã‚€
        credentials = json.loads(credentials_json_str)
        gc = gspread.service_account_from_dict(credentials)
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰JSONã‚’èª­ã¿è¾¼ã‚€
        # ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ 'key.json' ã«å¤‰æ›´
        gc = gspread.service_account(filename='key.json')

    for attempt in range(5):
        try:
            sh = gc.open_by_key(spreadsheet_id)
            try:
                worksheet = sh.worksheet(worksheet_name)
            except gspread.exceptions.WorksheetNotFound:
                worksheet = sh.add_worksheet(title=worksheet_name, rows="1", cols="4")
                worksheet.append_row(['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])

            existing_data = worksheet.get_all_values()
            existing_urls = set(row[1] for row in existing_data[1:] if len(row) > 1)

            new_data = [[a['ã‚¿ã‚¤ãƒˆãƒ«'], a['URL'], a['æŠ•ç¨¿æ—¥'], a['å¼•ç”¨å…ƒ']] for a in articles if a['URL'] not in existing_urls]
            if new_data:
                worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
                print(f"âœ… {len(new_data)}ä»¶ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            else:
                print("âš ï¸ è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        except gspread.exceptions.APIError as e:
            print(f"âš ï¸ Google API Error (attempt {attempt + 1}/5): {e}")
            time.sleep(5 + random.random() * 5)

    raise RuntimeError("âŒ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ5å›è©¦è¡Œã—ã¦ã‚‚æˆåŠŸã›ãšï¼‰")

if __name__ == "__main__":
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    spreadsheet_id_from_env = os.environ.get('SPREADSHEET_ID')
    spreadsheet_id = spreadsheet_id_from_env if spreadsheet_id_from_env else "1ZqRekcKkUUoVxZuO8hrWRWwTauyEk8kD_NmV5IZy02w"

    print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆID: {spreadsheet_id}")
    print(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {KEYWORD}")
    
    # Google News
    print("\n--- Google News ---")
    google_news_articles = get_google_news_with_selenium(KEYWORD)
    if google_news_articles:
        write_to_spreadsheet(google_news_articles, spreadsheet_id, "Google")

    # Yahoo! News
    print("\n--- Yahoo! News ---")
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD)
    if yahoo_news_articles:
        write_to_spreadsheet(yahoo_news_articles, spreadsheet_id, "Yahoo")

    # MSN News
    print("\n--- MSN News ---")
    msn_news_articles = get_msn_news_with_selenium(KEYWORD)
    if msn_news_articles:
        write_to_spreadsheet(msn_news_articles, spreadsheet_id, "MSN")
